{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrologyUnbound/SIOP_ML_2024_Discord/blob/main/colabs/Clarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rating Item Clarity Notebook\n",
        "This notebook is designed to tackle the challenge of predicting the average clarity rating for each personality item based on responses.  \n",
        "\n",
        "## Challenge Description\n",
        "Respondents rated the clarity of personality test items using a 7-point scale from 1 = extremely unclear to 7 = extremely clear.  \n"
      ],
      "metadata": {
        "id": "F6SSTTjSjTnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBcwNLEgeuDy"
      },
      "outputs": [],
      "source": [
        "!pip install pandas langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "QCTcFUeDkOl-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clarity_train_data = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/train/clarity_train.csv\")\n",
        "clarity_test_data = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/test/clarity_test_public.csv\")"
      ],
      "metadata": {
        "id": "Qt_NcSn_kUf3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually add env key to the `api_key` argument\n",
        "llm = ChatOpenAI(api_key= userdata.get('OPENAI_API_KEY'), model_name=\"gpt-3.5-turbo\", temperature=0.0)"
      ],
      "metadata": {
        "id": "vJd6wtHfkXVX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "Your task is to predict the average clarity rating for each item based on the responses.\n",
        "Respondents rated the clarity of personality test items using a 7-point scale from 1 = extremely unclear to 7 = extremely clear.\n",
        "The output should not make up information and not reference these given instructions or context; only output the answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vTMh2fuDnJiS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_examples_clarity(dataset_row):\n",
        "    # Extract personality_item and clarity from each row of the dataset\n",
        "    personality_item = dataset_row[\"personality_item\"]\n",
        "    clarity = dataset_row[\"clarity\"]\n",
        "\n",
        "    return [{\"personality_item\": personality_item, \"clarity\": clarity}]\n",
        "\n",
        "def create_examples_clarity(dataset):\n",
        "    # Extract examples from the first three rows of the dataset\n",
        "    examples = []\n",
        "    for i in range(3):\n",
        "        examples.extend(extract_examples_clarity(dataset.loc[i]))\n",
        "\n",
        "    return examples\n",
        "\n",
        "def create_example_prompt_clarity():\n",
        "    # Create a formatter for the examples\n",
        "    example_prompt = PromptTemplate(\n",
        "        input_variables=[\"personality_item\", \"clarity\"],\n",
        "        template=\"Personality Item: {personality_item}\\nClarity: {clarity}\"\n",
        "    )\n",
        "\n",
        "    return example_prompt\n",
        "\n",
        "def create_template_clarity(dataset):\n",
        "    # Generate a few shot prompt template\n",
        "    examples = create_examples_clarity(dataset)\n",
        "    template = FewShotPromptTemplate(\n",
        "        examples=examples,\n",
        "        example_prompt=create_example_prompt_clarity(),\n",
        "        prefix=instructions,\n",
        "        suffix=\"Personality Item: {input}\",\n",
        "        input_variables=[\"input\"],\n",
        "    )\n",
        "\n",
        "    return template"
      ],
      "metadata": {
        "id": "q2Opmj0_nQdD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = create_template_clarity(clarity_train_data).format(input=clarity_test_data[\"personality_item\"][0])\n",
        "example_prompt"
      ],
      "metadata": {
        "id": "B_w5p7gdnUK1",
        "outputId": "efa37212-090c-4f70-d568-b3d16d97ca40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYour task is to predict the average clarity rating for each item based on the responses. \\nRespondents rated the clarity of personality test items using a 7-point scale from 1 = extremely unclear to 7 = extremely clear. \\nThe output should not make up information and not reference these given instructions or context; only output the answer.\\n\\n\\nPersonality Item: Am considered well-off financially.\\nClarity: 3.421052631578947\\n\\nPersonality Item: Make problems bigger than they are.\\nClarity: 6.545454545454546\\n\\nPersonality Item: Judge people by their appearance.\\nClarity: 6.545454545454546\\n\\nPersonality Item: Want to be in charge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(example_prompt)"
      ],
      "metadata": {
        "id": "6VHOfnk9nYuc",
        "outputId": "d56869bf-590b-4d95-8008-44ee6f2e69e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='4.837837837837838', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 153, 'total_tokens': 160}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRrxA23lnbP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}