{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrologyUnbound/SIOP_ML_2024_Discord/blob/main/colabs/Fairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Fairness Perceptions Notebook\n",
        "This notebook is designed to tackle the challenge of identifying which policy received the majority vote as the fairer option.\n",
        "\n",
        "## Challenge Description\n",
        "Respondents compared two organizational policies and voted on which was fairest.  "
      ],
      "metadata": {
        "id": "4hul-RCw2W4C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHFekqEje3Hv"
      },
      "outputs": [],
      "source": [
        "!pip install pandas langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "JNl02BEv21D8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_train_data = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/train/fairness_train.csv\")\n",
        "fairness_test_data = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/test/fairness_test_public.csv\")"
      ],
      "metadata": {
        "id": "_kuGBEmV27N_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually add env key to the `api_key` argument\n",
        "llm = ChatOpenAI(api_key= userdata.get('OPENAI_API_KEY'), model_name=\"gpt-3.5-turbo\", temperature=0.0)"
      ],
      "metadata": {
        "id": "QS-aViY-3D34"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "Given the first and second options, your task is to predict which option received the majority vote as the fairer option. T\n",
        "he output should not make up information and not reference these given instructions or context; only output the answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FvMm6vd53Hm7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_examples_fairness(dataset_row):\n",
        "    # Extract first_option, second_option, and majority_vote from each row of the dataset\n",
        "    first_option = dataset_row[\"first_option\"]\n",
        "    second_option = dataset_row[\"second_option\"]\n",
        "    majority_vote = dataset_row[\"majority_vote\"]\n",
        "\n",
        "    return [{\"first_option\": first_option, \"second_option\": second_option, \"majority_vote\": majority_vote}]\n",
        "\n",
        "def create_examples_fairness(dataset):\n",
        "    # Extract examples from the first three rows of the dataset\n",
        "    examples = []\n",
        "    for i in range(3):\n",
        "        examples.extend(extract_examples_fairness(dataset.loc[i]))\n",
        "\n",
        "    return examples\n",
        "\n",
        "def create_example_prompt_fairness():\n",
        "    # Create a formatter for the examples\n",
        "    example_prompt = PromptTemplate(\n",
        "        input_variables=[\"first_option\", \"second_option\", \"majority_vote\"],\n",
        "        template=\"First Option: {first_option}\\nSecond Option: {second_option}\\nMajority Vote: {majority_vote}\"\n",
        "    )\n",
        "\n",
        "    return example_prompt\n",
        "\n",
        "def create_template_fairness(dataset_row):\n",
        "    # Generate a few shot prompt template\n",
        "    examples = create_examples_fairness(dataset_row)\n",
        "\n",
        "    template = FewShotPromptTemplate(\n",
        "        examples=examples,\n",
        "        example_prompt=create_example_prompt_fairness(),\n",
        "        prefix=instructions,\n",
        "        suffix=\"First Option: {first_option}\\nSecond Option: {second_option}\",\n",
        "        input_variables=[\"first_option\", \"second_option\"],\n",
        "    )\n",
        "\n",
        "    return template"
      ],
      "metadata": {
        "id": "gYYmhp6D3Mu0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create templates and fetch the first_option and second_option from the test dataset\n",
        "formatted_prompts = []\n",
        "\n",
        "for i in range(len(fairness_train_data)):\n",
        "    prompt_template = create_template_fairness(fairness_train_data)\n",
        "\n",
        "    for j in range(len(fairness_test_data)):\n",
        "\n",
        "        formatted_prompt = prompt_template.format(first_option=fairness_test_data.loc[j, \"first_option\"], second_option=fairness_test_data.loc[j, \"second_option\"])\n",
        "        formatted_prompts.append(formatted_prompt)"
      ],
      "metadata": {
        "id": "WlCjrNNA3TEv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(formatted_prompts[0])"
      ],
      "metadata": {
        "id": "_gvm0uW83VAO",
        "outputId": "9ca28ebe-2ad5-4f23-9c95-180b5dc9ff93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Majority Vote: first', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 231, 'total_tokens': 236}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}