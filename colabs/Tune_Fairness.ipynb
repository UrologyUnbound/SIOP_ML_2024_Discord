{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrologyUnbound/SIOP_ML_2024_Discord/blob/main/colabs/Tune_Fairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGCbakCrEPvQ"
      },
      "source": [
        "# Identifying Fairness Perceptions Model Fine Tuning\n",
        "\n",
        "The goal of this notebook is to fine tune the OpenAI GPT 3.5 model to be able to correctly identify which of two organizational policies received the majority vote as the fairer option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtkAbc8uF3i6"
      },
      "source": [
        "## Inputs and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHKf_hXSN85p",
        "outputId": "6398b14e-06fd-4991-9ac5-da5ef8ff4c79"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tiktoken openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhCfpbtlGAHM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import tiktoken\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "client = openai.OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEqFleTLfyLO"
      },
      "outputs": [],
      "source": [
        "#Estimated token counter\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMhy7lRJN_et"
      },
      "source": [
        "## Data Import and Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "G8akfGHiGXCU",
        "outputId": "66b4a353-9abe-4d06-aa58-eab43010e3fe"
      },
      "outputs": [],
      "source": [
        "df_fairness = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/train/fairness_train.csv\")\n",
        "df_fairness.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yPaLpkA1HZxG",
        "outputId": "e37aafad-1556-4c4c-ae55-43ff5e91b09e"
      },
      "outputs": [],
      "source": [
        "# Split the df_fairness dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_fairness[[\"first_option\", \"second_option\"]], df_fairness[\"majority_vote\"], random_state=42, test_size=0.2)\n",
        "\n",
        "train_data = pd.concat([x_train, y_train], axis=1)\n",
        "test_data = pd.concat([x_test, y_test], axis=1)\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9mDNYK0L2XG",
        "outputId": "47d8bbc3-4a51-4fdb-f002-b3bf2055b098"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Given the first and second options, your task is to predict which option received the majority vote as the fairer option. The output should not make up information and not reference these given instructions or context; only output the answer.\n",
        "\"\"\"\n",
        "\n",
        "def create_user_message(row):\n",
        "    # Create the user message with the first and second options\n",
        "    user_message = f\"First Option: {row['first_option']}\\nSecond Option: {row['second_option']}\"\n",
        "    return user_message\n",
        "\n",
        "def prepare_example_conversation(row):\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    user_message = create_user_message(row)\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Use the majority_vote column as the assistant's message\n",
        "    messages.append({\"role\": \"assistant\", \"content\": row[\"majority_vote\"]})\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "pprint(prepare_example_conversation(train_data.iloc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shiKN4dPNjf1",
        "outputId": "fbb6f744-e55f-4559-d963-5e81123bde2e"
      },
      "outputs": [],
      "source": [
        "training_json = train_data.apply(prepare_example_conversation, axis=1).tolist()\n",
        "test_json = test_data.apply(prepare_example_conversation, axis=1).tolist()\n",
        "\n",
        "\n",
        "for example in training_json[:5]:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HZFZcIGOsGr"
      },
      "outputs": [],
      "source": [
        "def write_jsonl(data_list: list, filename: str) -> None:\n",
        "    with open(filename, \"w\") as out:\n",
        "        for ddict in data_list:\n",
        "            jout = json.dumps(ddict) + \"\\n\"\n",
        "            out.write(jout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CspPh8nJOyAu"
      },
      "outputs": [],
      "source": [
        "training_file_name = \"tmp_fairness_finetune_training.jsonl\"\n",
        "write_jsonl(training_json, training_file_name)\n",
        "\n",
        "testing_file_name = \"tmp_fairness_finetune_testing.jsonl\"\n",
        "write_jsonl(test_json, testing_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1eYX5vaPKGq",
        "outputId": "e3b7b1d4-c35b-467a-9f85-5c1893147998"
      },
      "outputs": [],
      "source": [
        "!head -n 5 tmp_fairness_finetune_training.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEzRcfLnf9Lw"
      },
      "source": [
        "### Pre-Tuning Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5-zgm6HeWUu",
        "outputId": "6ac29060-a985-453c-de82-41234b1648a1"
      },
      "outputs": [],
      "source": [
        "# Format error checks - Training set\n",
        "with open(\"/content/tmp_fairness_finetune_testing.jsonl\", 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK9EdNDofBYq",
        "outputId": "362fa14e-d805-47b0-a42d-f74abfa134d2"
      },
      "outputs": [],
      "source": [
        "# Format error checks - Training set\n",
        "with open(\"/content/tmp_fairness_finetune_training.jsonl\", 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9CqjO0zfnZT",
        "outputId": "7e9affd3-aaeb-4335-91cc-eeead7e258a6"
      },
      "outputs": [],
      "source": [
        "# Warnings and tokens counts\n",
        "with open(\"/content/tmp_fairness_finetune_testing.jsonl\", 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBNhJKoSgVZM",
        "outputId": "af686439-baa4-451b-ec2f-2fde293823a9"
      },
      "outputs": [],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "print(f\"Estimated training cost ~${((n_epochs * n_billing_tokens_in_dataset)/1000)*.0080}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kPzYKTDPZTk",
        "outputId": "7eaaced7-69a1-407b-bb6e-3db3758200ab"
      },
      "outputs": [],
      "source": [
        "with open(training_file_name, \"rb\") as training_fd:\n",
        "    training_response = client.files.create(\n",
        "        file=training_fd, purpose=\"fine-tune\"\n",
        "    )\n",
        "\n",
        "training_file_id = training_response.id\n",
        "\n",
        "with open(testing_file_name, \"rb\") as validation_fd:\n",
        "    validation_response = client.files.create(\n",
        "        file=validation_fd, purpose=\"fine-tune\"\n",
        "    )\n",
        "validation_file_id = validation_response.id\n",
        "\n",
        "print(\"Training file ID:\", training_file_id)\n",
        "print(\"Validation file ID:\", validation_file_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmi4TAoxdyQ1"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUvyai4FYMUb",
        "outputId": "d616af59-6823-4c5f-b417-139eddbb453a"
      },
      "outputs": [],
      "source": [
        "# Only Run this cell when wanting to create a new fine-tuning job, otherwise you will be paying to redo work\n",
        "\n",
        "#Uncomment the below code when wanting to run a new fine-tuning job\n",
        "# response = client.fine_tuning.jobs.create(\n",
        "#     training_file=training_file_id,\n",
        "#     validation_file=validation_file_id,\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     suffix=\"fairness_tuned_v1\",\n",
        "# )\n",
        "\n",
        "job_id = response.id\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvT6IMZnk2OG",
        "outputId": "f8e2fd0d-2a5b-4e65-f8d2-70bbdda48ddc"
      },
      "outputs": [],
      "source": [
        "#Check Job Status\n",
        "response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)\n",
        "print(\"Trained Tokens:\", response.trained_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt70HufqlIQ_",
        "outputId": "86fb0e95-28ae-4b7a-fff3-824145758f8c"
      },
      "outputs": [],
      "source": [
        "#Track Fine-Tuning Endpoints\n",
        "response = client.fine_tuning.jobs.list_events(job_id)\n",
        "\n",
        "events = response.data\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biVRMiJNlaDP",
        "outputId": "871c77da-42d2-4611-a805-349796caadcf"
      },
      "outputs": [],
      "source": [
        "# When job is done, run to fets fine-tuned model id\n",
        "response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "fine_tuned_model_id = response.fine_tuned_model\n",
        "\n",
        "if fine_tuned_model_id is None:\n",
        "    raise RuntimeError(\"Fine-tuned model ID not found. Your job has likely not been completed yet.\")\n",
        "\n",
        "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we7GsYT9mM0N"
      },
      "source": [
        "## Fine-Tuned Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f762jfvmSSi"
      },
      "outputs": [],
      "source": [
        "df_fairness_dev = pd.read_csv(\"https://raw.githubusercontent.com/UrologyUnbound/SIOP_ML_2024_Discord/main/data/dev/fairness_val_public.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgsmL4nJmQRA",
        "outputId": "4ecf9198-020e-4246-cd72-792667cab051"
      },
      "outputs": [],
      "source": [
        "test_row = df_fairness_dev.iloc[2]\n",
        "test_messages = []\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = create_user_message(test_row)\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "pprint(test_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj2TZFzGmg0I",
        "outputId": "ba976b95-c0a9-4f72-8c3d-4da4cfdb61ca"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOxUTTfL3+NS1eCixuLbOK6",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
